{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d918bb88-9075-40ad-b21f-28b72e86c487",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Preprocessing\n",
    "This notebook will walk through the concepts we saw today. Specifically, it presents\n",
    "\n",
    "- How to load data using pandas\n",
    "- How to split the data into train and test\n",
    "- How to use the differents transformers\n",
    "- A complete example on how to preprocess the data on a real dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d28059-e464-4a7b-bb45-5be805ed1c44",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8053d4d-fee0-48ae-a717-80cb2e722b07",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "First of all, we will see how to load data using **pandas**. We will see 2 examples: loading a local file and loading a dataset from a URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73335c54-78e1-4e65-8aa4-e594ef610856",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "titanic_from_file = pandas.read_csv(\"titanic.csv\")\n",
    "titanic_from_url = pandas.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0101ad32-b22d-4c2b-bdc2-e1eae5178c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_from_file.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74c079f-c009-4238-9f6e-29aaa34e02fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_from_url.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb534f57-cf0b-4cd8-8d6a-142d330efe64",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Split the data into train and test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef29ba0-9a45-459f-9120-940b814a9700",
   "metadata": {},
   "source": [
    "\n",
    "The next step once we have the data loaded into memory using **pandas** is to obtain our train and test datasets. Remember that this process uses random selection, so for reproducibility we will set a seed. This means that each time we call this method it will return the same selection.\n",
    "\n",
    "First, we will separate the data into features (X) and labels (y), as this dataset is usually used for classification (i.e. supervised learning), and contains a column with the label *Survived*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8424b0-47d6-481a-b854-37cd992f31df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use drop to get the dataset without a specific column. If the parameter\n",
    "# inplace is not set to True, it will not modify the original data\n",
    "X = titanic_from_file.drop(\"Survived\", axis=1)\n",
    "y = titanic_from_file[\"Survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a396135f-51ac-4ce4-b260-5cb3d27bb978",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03113db6-a2d4-4df1-974b-9e4f200b3f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2fe8cf-7bd1-46ae-b9c8-2640edcdbd57",
   "metadata": {},
   "source": [
    "Now we are ready to split the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd186c06-a014-43ff-a365-824fdff37ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "test_size_pctg = 0.2\n",
    "seed = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size_pctg, random_state=seed)\n",
    "\n",
    "print(X.shape, X_train.shape, X_test.shape)\n",
    "print(y.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19b9ad2-c7ee-4a26-987a-53fc3af936f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Transforming data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bfc14e-237f-477d-885d-b50f12933949",
   "metadata": {},
   "source": [
    "\n",
    "Here we show a basic example of the transformers explained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1246598f-b5b3-493c-9a3f-3b5605bf707a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6644d2ec-359c-497c-9f68-301b1f7b723e",
   "metadata": {
    "tags": []
   },
   "source": [
    "First, we see the normalization process using the `StandardScaler`. This will convert each column into a new column with standard deviation of 1 and mean of 0.\n",
    "\n",
    "As the standard scaler will apply the function:\n",
    "$$\n",
    "    z_i = \\frac{x_i - \\mu}{\\sigma}\n",
    "$$\n",
    "to scale the values, first it must learn this $\\mu$ (mean) and $\\sigma$ (std. deviation). To learn this values (parameters) we will **fit** the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f7622c-cee6-443f-a142-1cd35d691962",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pandas.DataFrame({\n",
    "    \"column1\": [1, 2, 4, 5, 6, 7, 2, 1, 3],\n",
    "    \"column2\": [-1, 2, 1, 1, 3, 4, 5, 6, 7],\n",
    "})\n",
    "\n",
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75d2a65-4aea-46fa-8f20-666b39b279f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(dataset)  # Learn std.dev and mean\n",
    "\n",
    "new_X = scaler.transform(dataset)\n",
    "# Note that the scaler will return a numpy array\n",
    "new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75db0944-d70e-4131-a432-4ffd1c766fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean for each column:\", new_X.mean(axis=0))\n",
    "print(\"Standard deviation for each column:\", new_X.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad408f47-27fe-49cd-b2af-f50082df8ca0",
   "metadata": {},
   "source": [
    "Finally, let's see that this transformer allows us to recover the original data using `inverse_transform`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c53f712-032f-4566-b829-662ae70ab932",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler.inverse_transform(new_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6777e4-460c-4738-ae11-d0c75852bfb2",
   "metadata": {},
   "source": [
    "As an exercise, try to implement the StandardScaler by yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e72610-dd01-4853-a4ac-8912cba568ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class MyStandardScaler(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X):\n",
    "        if isinstance(X, pandas.DataFrame):\n",
    "            X = X.values\n",
    "        \n",
    "        raise NotImplementedError\n",
    "        # TODO get the mean and the standard deviation for each column\n",
    "        self.means_ = None\n",
    "        self.std_dev_ = None\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if isinstance(X, pandas.DataFrame):\n",
    "            X = X.values\n",
    "        \n",
    "        # Use the mean and the std deviation of each column to scale\n",
    "        # the data in X\n",
    "        raise NotImplementedError\n",
    "        \n",
    "my_X = MyStandardScaler().fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432adf6e-a911-4651-9fda-5f1dd1b0aaaf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5d5da3-3869-4d2c-af2e-f4b13bd71fdd",
   "metadata": {},
   "source": [
    "The next transformer we explained is the *bins discretizer*. Experiment with the different parameters available (see [KBinsDiscretizer](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50abaf18-6b8a-4189-b04a-771d0a7a455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "discretizer = KBinsDiscretizer(n_bins=3, encode=\"ordinal\")\n",
    "discretizer.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0972b7-e8fc-4e1e-8306-953cf6df94ff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abe3315-4f33-4458-bd0b-4791bbfb1c2e",
   "metadata": {},
   "source": [
    "Finally, let's see the encoders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95661fe-65b3-42c1-8038-9fcd9e2ee838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "ohe = OneHotEncoder(sparse=False)\n",
    "print(ohe.fit_transform(dataset))\n",
    "print(ohe.categories_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e7a1b5-7e31-4ea1-87f3-a193d14bdbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "dataset_categorical = pandas.DataFrame({\n",
    "    \"col1\": [\"cat\", \"dog\", \"bird\", \"cat\", \"bird\", \"cat\"],\n",
    "})\n",
    "\n",
    "label_enc = LabelEncoder()\n",
    "print(label_enc.fit_transform(dataset_categorical))\n",
    "print(label_enc.classes_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea5111e-630d-4064-8f8e-8ee367931d4c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6f217d-c39a-4b43-9c5a-bf44a79838c5",
   "metadata": {},
   "source": [
    "\n",
    "In some cases, we might not be able to determine which features are worth or not to be used in a model manually. For those cases, we can use statistics and unsupervised learning to find relations between the features and the label in our training set, and then select some features based on this new information.\n",
    "\n",
    "An example of this technique is the SelectKBest method in sklearn, where we determine which are the best $K$ features based on univariate statistical tests, such as $\\chi^2$ (chi squared)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1de290a-b160-4fdb-a927-9f318076fe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "selector = SelectKBest(chi2, k=2)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "print(X_new.shape)\n",
    "print(selector.scores_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673a8831-1c0f-4abd-959c-c3d18b8dd8f5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data scrubbing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58139908-8c5b-4280-b348-3109a4de3921",
   "metadata": {},
   "source": [
    "Below we have a real example on how to preprocess a dataset. We will use the[Titanic dataset](https://hbiostat.org/data/repo/titanic.txt).\n",
    "\n",
    "The problem we try to solve is to determine if a Titanic's passenger would have survived, given the age, passenger class, and sex.\n",
    "\n",
    "The attributes are:\n",
    "- idx (we use it as the dataframe index using index_col=0)\n",
    "- Class\n",
    "- Survived (1=True / 0=False)\n",
    "- Name\n",
    "- Age\n",
    "- Embarked\n",
    "- Destination\n",
    "- Room\n",
    "- Ticket\n",
    "- Boat\n",
    "- Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7719ba39-06f6-480b-ac25-6735b53af46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "titanic = pandas.read_csv(\"https://hbiostat.org/data/repo/titanic.txt\", index_col=0)\n",
    "\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6aa719-7e0e-40f7-b0cf-eb2d56ad38e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_X = titanic.drop(\"survived\", axis=1)\n",
    "titanic_y = titanic[\"survived\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    titanic_X, titanic_y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052d8be7-f587-4f2f-a735-61940d86b6cb",
   "metadata": {},
   "source": [
    "Let's select some attributes we will use for learning:\n",
    "- Passenger class (pclass)\n",
    "- Age\n",
    "- Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f618608-a9b9-403a-81df-29a7393ee433",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[[\"pclass\", \"age\", \"sex\"]]\n",
    "\n",
    "print(X_train.shape)\n",
    "X_train.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1d4f0d-811c-4f13-a2ed-134938f72048",
   "metadata": {},
   "source": [
    "We see that for the age attribute we have 448 rows of 1050 that have missing values. We can approach this using any of the techniques we explained:\n",
    "- Replacing it with the mean value\n",
    "- Replacing it using KNN\n",
    "- Replacing it randomly\n",
    "- Deleting the records\n",
    "\n",
    "The example uses the SimpleImputer to show how this can be done, but as we know it is easy to replace one transformer for another using *sklearn*, so try to change it and experiment with the other methods. Remember that other methods might require the data as integers only. In those cases, for now, you can apply the fit and transform only to a specific column using something like:\n",
    "```python\n",
    "dest = titanic_X.copy()\n",
    "columns_to_input = [\"age\"]\n",
    "dest[columns_to_input] = imputer.fit_transform(titanic_X[columns_to_input] \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770d5618-54f7-4caa-8abb-19d4c2a999ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "# First learn the parameters (in the simple imputer this is the\n",
    "# most frequent value) for each column \n",
    "imputer.fit(X_train)\n",
    "# Then \"impute\" the values\n",
    "X_train_imputed = imputer.transform(X_train)\n",
    "\n",
    "# \"Reconvert\" it to a dataframe\n",
    "X_train_imputed = pandas.DataFrame(X_train_imputed,\n",
    "                                   columns=X_train.columns,\n",
    "                                   index=X_train.index)\n",
    "\n",
    "# Check we have no more nans\n",
    "print(X_train_imputed.shape)\n",
    "X_train_imputed.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919a87ed-45fa-49bf-a923-e9b5beb8e7ab",
   "metadata": {},
   "source": [
    "Once we have the values imputed, then we can prepare it for our model. We will use sklearn decision trees. This model expects as input a list of *real-valued* features, as the decisions will be **Feature <= value**.\n",
    "\n",
    "As we have categorical data, we have to convert it to integer or real values. We could use either a *LabelEncoder* or a *OneHotEncoder*.\n",
    "\n",
    "The example shows a *LabelEncoder* for the feature *pclass*, but remember that as the category might not have a natural ordering, a *OneHotEncoder* might be better as the *LabelEncoder* will map each category to an integer between $[0, K-1]$, where $K$ is the number of categories for each column, thus introducing an ordering between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff897019-f159-48f2-9dfd-34a9591033cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "\n",
    "X_train_encoded = X_train_imputed.copy()\n",
    "\n",
    "pclass_encoder = LabelEncoder()\n",
    "pclass_encoder = pclass_encoder.fit(X_train_imputed[\"pclass\"])\n",
    "X_train_encoded[\"pclass\"] = pclass_encoder.transform(X_train_imputed[\"pclass\"])\n",
    "\n",
    "# [...] Exercise: encode the age using a OneHotEncoder (set the parameter sparse to false)\n",
    "# Remember that the OHE works with multiple columns as input, while the LabelEncoder\n",
    "# works with a single column input, so the calls to fit and transform might be a bit\n",
    "# different.\n",
    "# Hint: as the OneHotEncoder will return mutliple columns for each column\n",
    "# you can drop the original column in X_train_encoded and use something like:\n",
    "# >>> new_columns = list(map(lambda s: \"sex_\" + s, list(encoder.categories_[0])))\n",
    "# >>> X_train_encoded[new_columns] = encoder.transform(...)\n",
    "\n",
    "X_train_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9b367f-2274-4af0-9a81-e248dcb9c732",
   "metadata": {},
   "source": [
    "We are now ready to fit our first model with clean data. We first separate it into train and test, and then fit the classifier. Finally, we check the score of the model in both train and test, to see if our model overfits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f84b641-9c8c-4cd9-b794-a935c98d3411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# NOTE: Once you have encoded the column sex using OHE, you can comment the following line\n",
    "X_train_encoded = X_train_encoded[[\"pclass\", \"age\"]]\n",
    "\n",
    "\n",
    "model = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3, min_samples_leaf=5)\n",
    "model.fit(X_train_encoded, y_train)\n",
    "\n",
    "print(\"Score in train:\", model.score(X_train_encoded, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a507d8c8-2215-4952-9417-5f6053a1db13",
   "metadata": {},
   "source": [
    "In order to get the score for the test dataset, we have to apply the same preprocessing as we dit to the train dataset. **Remember that the test dataset cannot modify any parameter of the entire process. This means that we cannot call any method that fits anything**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3a9014-d874-4719-b735-d68fe44e4d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO preprocess the dataset\n",
    "X_test_prepared = X_test\n",
    "\n",
    "print(\"Score in test:\", model.score(X_test_prepared, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bfa27b-2d7a-4df4-a5d9-728ae9866aae",
   "metadata": {},
   "source": [
    "As a bonus, we will inspect the built tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9123df9-80a3-44c6-b853-0439a32a31f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "tree.plot_tree(model, feature_names=X_train.columns)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
